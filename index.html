<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Camera + AI (Transformers.js)</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<style>
  body {
    background:#020617;
    color:#e5e7eb;
    font-family: system-ui;
    display:flex;
    justify-content:center;
    align-items:center;
    height:100vh;
    margin:0;
  }
  .app {
    width:100%;
    max-width:420px;
    padding:15px;
  }
  video {
    width:100%;
    border-radius:12px;
  }
  button {
    width:100%;
    margin-top:12px;
    padding:12px;
    border:none;
    border-radius:8px;
    background:#6366f1;
    color:white;
    font-size:16px;
  }
  .result {
    margin-top:12px;
    text-align:center;
    font-size:18px;
  }
</style>
</head>

<body>
<div class="app">
  <h2>ðŸ“· Camera AI Analyzer</h2>

  <video id="video" autoplay playsinline></video>

  <button id="snap">Analyze Frame</button>

  <div class="result" id="result">Loading AIâ€¦</div>
</div>

<script type="module">
import { pipeline } from "https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.1";

const video = document.getElementById("video");
const result = document.getElementById("result");
const snapBtn = document.getElementById("snap");

let classifier;

// Open camera
navigator.mediaDevices.getUserMedia({
  video: { facingMode: "environment" }
}).then(stream => {
  video.srcObject = stream;
});

// Load vision model
classifier = await pipeline(
  "image-classification",
  "Xenova/mobilenet_v2_1.0_224"
);

result.textContent = "âœ… Model ready";

// Capture frame & analyze
snapBtn.onclick = async () => {
  result.textContent = "ðŸ¤– Analyzingâ€¦";

  const canvas = document.createElement("canvas");
  canvas.width = video.videoWidth;
  canvas.height = video.videoHeight;

  const ctx = canvas.getContext("2d");
  ctx.drawImage(video, 0, 0);

  const prediction = await classifier(canvas);

  result.innerHTML =
    `ðŸ§  ${prediction[0].label}<br>ðŸŽ¯ ${(prediction[0].score*100).toFixed(1)}%`;
};
</script>
</body>
</html>
